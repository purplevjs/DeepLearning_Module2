# Deep Learning Module 2: Sentence Analysis Models

This project provides a suite of models for analyzing sentences, extracting importance ratings, and determining long-term storage classifications.

## Setup and Running the Project
Requirements
*Python 3.7+
*PyTorch
*Transformers (Hugging Face)
*scikit-learn
*pandas
*matplotlib
*seaborn

Install the required packages:
pip install torch transformers pandas scikit-learn matplotlib seaborn

## Data Source
Generated by Chagpt, Claude and Deepseek.

## Previous Research
No previous research yet.

## Data Structure
The project expects CSV files in the ./Data directory with columns for sentences, importance ratings, and long-term storage classification.

## Running the Project
The simplest way to run the project is using the main script:
python main.py

This will present a menu with options to:
Train the RoBERTa model (standard training)
Train the RoBERTa model (with cross-validation)
Train the SVM model
Train the naive word-matching approach
Train all models

## Running Individual Model Training Scripts
To run individual training scripts directly:
RoBERTa Model (Standard Training):
python scripts/Final_Train.py

RoBERTa Model (Cross-Validation):
python scripts/Train.py

SVM Model:
python scripts/machine_learning_svm.py

Naive Approach:
python scripts/naive_approach.py

## Project Structure and File Descriptions

### Core Scripts
main.py: Main entry point with a user-friendly interface to select training methods.
setup.py: Contains functions for initializing and running different model training processes.

### Data Processing
scripts/Data_preprocess.py: Handles data preprocessing, loading CSV files from the Data folder, and preparing them for model training. It standardizes column names and normalizes binary classification values.
scripts/Dataset_Class.py: Defines the custom TextDataset class that handles text data for PyTorch models. It converts text inputs into tokenized tensors that can be consumed by the deep learning models.

### Models
scripts/Model_Class.py: Implements the DistinguishModel class using RoBERTa as the backbone. It features:
Separate heads for regression (importance) and classification (long-term storage)
Residual connections for improved feature extraction
Selective layer freezing for efficient fine-tuning

### Training Scripts
scripts/Final_Train.py: Implements standard training for the RoBERTa model with early stopping, learning rate scheduling, and best model saving.
scripts/Train.py: Implements k-fold cross-validation training for the RoBERTa model, which provides more robust performance evaluation across different data splits.
scripts/machine_learning_svm.py: Trains Support Vector Machine models for both classification and regression tasks using TF-IDF text representation. It generates comprehensive evaluation metrics and visualizations.
scripts/naive_approach.py: Implements a simple word-matching approach for baseline comparison, identifying relevant keywords to predict long-term storage.

### Training Outputs
Each training script generates:
Model files: Saved models that can be loaded for inference
Performance metrics: Accuracy, precision, recall, F1-score for classification; MSE/R¬≤ for regression
Visualizations: Loss curves, confusion matrices, ROC curves, etc.
Prediction results: CSV files with true and predicted values

### Project Workflow
1. Data is preprocessed and normalized
2. Models are initialized with appropriate architectures
3. Training is performed with task-specific loss functions
4. Performance is evaluated with appropriate metrics
5. Results are saved for comparison and future use
This project demonstrates multiple approaches to sentence analysis, from simple rule-based methods to complex deep learning architectures, enabling comparison of different techniques for the same tasks.


# üìå SmartForget: Embedding-Based Redundancy Removal for AI Memory Cleanup

## üîç Project Overview

As AI systems continuously accumulate user data to provide personalized and contextual support, managing the storage of this information becomes a major challenge. AI memory databases are expensive and finite, requiring intelligent strategies to periodically prune redundant or outdated data.

**SmartForget** is a lightweight memory cleanup system that uses vector embeddings and cosine similarity to identify and remove semantically redundant memory entries‚Äîmimicking the way humans forget unimportant details while preserving core information.

---

## üß† Problem Statement

- AI systems retain logs like:
  - ‚ÄúI had a headache today.‚Äù
  - ‚ÄúBirthday is Jan 24.‚Äù
- Not all memories are equally valuable long-term.
- Temporary or repetitive entries consume space without adding lasting value.
- We need a system that can **automatically detect and discard low-value memory entries** to optimize performance and storage.

---
---

## üîÑ Data Processing Pipeline

1. **Load & Clean Data**
   - Merged `train.csv` and `test.csv`
   - Removed NaNs and irrelevant columns

2. **Generate Embeddings**
   - Used `SentenceTransformer` (all-MiniLM-L6-v2) to encode all descriptions into vector embeddings
   - Each memory is now represented as a high-dimensional vector capturing semantic meaning

3. **Compute Similarity Matrix**
   - Used **cosine similarity** to compare memory entries
   - Built a full similarity matrix between all vectors

4. **Identify Redundant Entry**
   - Calculated total similarity score for each vector
   - Identified the entry with the highest cumulative similarity to others

5. **Memory Cleanup**
   - Removed the most redundant entry from the dataset

---

## üìà Model Approaches

- **Deep Learning Approach:**
  - SentenceTransformer embeddings (SBERT)
  - Rich, contextual understanding of language

- **Non-Deep Learning Baseline:**
  - TF-IDF + Cosine Similarity
  - Faster, but less accurate at detecting semantic overlap

- **Evaluation Metric:**
  - **Cosine Similarity Score**
  - Measures closeness between vector pairs

- **Comparison to Naive Baseline:**
  - Naive deletion (random or timestamp-based) vs. embedding-based deletion
  - Our method retained more unique, meaningful content


---

## ‚úÖ Results & Key Takeaways

- Embedding-based cleanup effectively reduces redundant entries
- Preserves critical, long-term information
- Scalable and easily integrated into real-world AI assistant architectures

---

## ‚öñÔ∏è Ethical Considerations

- Deletion must be explainable and transparent
- Avoid removing sensitive or long-term useful information
- Ensure fairness in what gets retained vs. forgotten
- Human oversight is recommended for edge cases

---

## üßë‚Äçüíª Authors

- Violet Suh ‚Äì Embedding strategy, similarity detection, and deletion logic
- Baze Bai ‚Äì Modeling evaluation, interface development, and performance testing
